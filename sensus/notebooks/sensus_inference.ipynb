{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.apis import inference_mono_3d_detector, init_model\n",
    "\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from os import path as osp\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose, pseudo_collate\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmengine.runner import load_checkpoint\n",
    "\n",
    "from mmdet3d.registry import DATASETS, MODELS\n",
    "from mmdet3d.structures import Box3DMode, Det3DDataSample, get_box_type\n",
    "from mmdet3d.structures.det3d_data_sample import SampleList\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = '/home/javier/sensus-loci/sensus/notebooks/002238.pkl'\n",
    "cam_type = 'CAM_BACK'\n",
    "pitch = 0.2031\n",
    "\n",
    "config_file = '/home/javier/sensus-loci/sensus/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d.py'\n",
    "checkpoint_file = '/home/javier/sensus-loci/work_dirs/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d/epoch_100.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/javier/sensus-loci/work_dirs/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d/epoch_100.pth\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_0.projs.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_0.nodes.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.projs.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.projs.1.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.nodes.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.nodes.1.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.1.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.2.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.1.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.2.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.projs.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.projs.1.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.nodes.0.conv is upgraded to version 2.\n",
      "11/02 17:07:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.nodes.1.conv is upgraded to version 2.\n"
     ]
    }
   ],
   "source": [
    "model = init_model(config_file, checkpoint_file, device=device)\n",
    "imgs = '/home/javier/sensus-loci/sensus/notebooks/002238.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(imgs, (list, tuple)):\n",
    "        is_batch = True\n",
    "else:\n",
    "        imgs = [imgs]\n",
    "        is_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config (path: /home/javier/sensus-loci/sensus/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d.py): {'dataset_type': 'KittiDataset', 'data_root': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/', 'class_names': ['Pedestrian', 'Cyclist', 'Car'], 'input_modality': {'use_lidar': False, 'use_camera': True}, 'metainfo': {'classes': ['Pedestrian', 'Cyclist', 'Car']}, 'backend_args': None, 'train_pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'LoadAnnotations3D', 'with_bbox': True, 'with_label': True, 'with_attr_label': False, 'with_bbox_3d': True, 'with_label_3d': True, 'with_bbox_depth': True}, {'type': 'RandomFlip3D', 'flip_ratio_bev_horizontal': 0.5}, {'type': 'RandomShiftScale', 'shift_scale': (0.2, 0.4), 'aug_prob': 0.3}, {'type': 'AffineResize', 'img_scale': (1920, 1080), 'down_ratio': 4}, {'type': 'Pack3DDetInputs', 'keys': ['img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths']}], 'test_pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'AffineResize', 'img_scale': (1920, 1080), 'down_ratio': 4}, {'type': 'Pack3DDetInputs', 'keys': ['img']}], 'eval_pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'Pack3DDetInputs', 'keys': ['img']}], 'train_dataloader': {'batch_size': 4, 'num_workers': 4, 'persistent_workers': True, 'sampler': {'type': 'DefaultSampler', 'shuffle': True}, 'dataset': {'type': 'KittiDataset', 'data_root': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/', 'ann_file': 'kitti_infos_train.pkl', 'data_prefix': {'img': 'training/image_2'}, 'pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'LoadAnnotations3D', 'with_bbox': True, 'with_label': True, 'with_attr_label': False, 'with_bbox_3d': True, 'with_label_3d': True, 'with_bbox_depth': True}, {'type': 'RandomFlip3D', 'flip_ratio_bev_horizontal': 0.5}, {'type': 'RandomShiftScale', 'shift_scale': (0.2, 0.4), 'aug_prob': 0.3}, {'type': 'AffineResize', 'img_scale': (1920, 1080), 'down_ratio': 4}, {'type': 'Pack3DDetInputs', 'keys': ['img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths']}], 'modality': {'use_lidar': False, 'use_camera': True}, 'load_type': 'fov_image_based', 'test_mode': False, 'metainfo': {'classes': ['Pedestrian', 'Cyclist', 'Car']}, 'box_type_3d': 'Camera', 'backend_args': None}}, 'val_dataloader': {'batch_size': 1, 'num_workers': 2, 'persistent_workers': True, 'drop_last': False, 'sampler': {'type': 'DefaultSampler', 'shuffle': False}, 'dataset': {'type': 'KittiDataset', 'data_root': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/', 'data_prefix': {'img': 'training/image_2'}, 'ann_file': 'kitti_infos_val.pkl', 'pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'AffineResize', 'img_scale': (1920, 1080), 'down_ratio': 4}, {'type': 'Pack3DDetInputs', 'keys': ['img']}], 'modality': {'use_lidar': False, 'use_camera': True}, 'load_type': 'fov_image_based', 'metainfo': {'classes': ['Pedestrian', 'Cyclist', 'Car']}, 'test_mode': True, 'box_type_3d': 'Camera', 'backend_args': None}}, 'test_dataloader': {'batch_size': 1, 'num_workers': 2, 'persistent_workers': True, 'drop_last': False, 'sampler': {'type': 'DefaultSampler', 'shuffle': False}, 'dataset': {'type': 'KittiDataset', 'data_root': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/', 'data_prefix': {'img': 'training/image_2'}, 'ann_file': 'kitti_infos_val.pkl', 'pipeline': [{'type': 'LoadImageFromFileMono3D', 'backend_args': None}, {'type': 'AffineResize', 'img_scale': (1920, 1080), 'down_ratio': 4}, {'type': 'Pack3DDetInputs', 'keys': ['img']}], 'modality': {'use_lidar': False, 'use_camera': True}, 'load_type': 'fov_image_based', 'metainfo': {'classes': ['Pedestrian', 'Cyclist', 'Car']}, 'test_mode': True, 'box_type_3d': 'Camera', 'backend_args': None}}, 'val_evaluator': {'type': 'KittiMetric', 'ann_file': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/kitti_infos_val.pkl', 'metric': 'bbox', 'backend_args': None}, 'test_evaluator': {'type': 'KittiMetric', 'ann_file': '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/kitti_infos_val.pkl', 'metric': 'bbox', 'backend_args': None}, 'vis_backends': [{'type': 'LocalVisBackend'}], 'visualizer': {'type': 'Det3DLocalVisualizer', 'vis_backends': [{'type': 'LocalVisBackend'}], 'name': 'visualizer'}, 'model': {'type': 'SMOKEMono3D', 'data_preprocessor': {'type': 'Det3DDataPreprocessor', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'bgr_to_rgb': True, 'pad_size_divisor': 32}, 'backbone': {'type': 'DLANet', 'depth': 34, 'in_channels': 3, 'norm_cfg': {'type': 'GN', 'num_groups': 32}, 'init_cfg': {'type': 'Pretrained', 'checkpoint': 'http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth'}}, 'neck': {'type': 'DLANeck', 'in_channels': [16, 32, 64, 128, 256, 512], 'start_level': 2, 'end_level': 5, 'norm_cfg': {'type': 'GN', 'num_groups': 32}}, 'bbox_head': {'type': 'SMOKEMono3DHead', 'num_classes': 3, 'in_channels': 64, 'dim_channel': [3, 4, 5], 'ori_channel': [6, 7], 'stacked_convs': 0, 'feat_channels': 64, 'use_direction_classifier': False, 'diff_rad_by_sin': False, 'pred_attrs': False, 'pred_velo': False, 'dir_offset': 0, 'strides': None, 'group_reg_dims': (8,), 'cls_branch': (256,), 'reg_branch': ((256,),), 'num_attrs': 0, 'bbox_code_size': 7, 'dir_branch': (), 'attr_branch': (), 'bbox_coder': {'type': 'SMOKECoder', 'base_depth': (28.01, 16.32), 'base_dims': ((0.88, 1.73, 0.67), (1.78, 1.7, 0.58), (3.88, 1.63, 1.53)), 'code_size': 7}, 'loss_cls': {'type': 'mmdet.GaussianFocalLoss', 'loss_weight': 1.0}, 'loss_bbox': {'type': 'mmdet.L1Loss', 'reduction': 'sum', 'loss_weight': 0.0033333333333333335}, 'loss_dir': {'type': 'mmdet.CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'loss_attr': None, 'conv_bias': True, 'dcn_on_last_conv': False, 'train_cfg': None, 'test_cfg': {'topK': 100, 'local_maximum_kernel': 3, 'max_per_img': 100}}, 'train_cfg': None, 'test_cfg': {'topK': 100, 'local_maximum_kernel': 3, 'max_per_img': 100}}, 'default_scope': 'mmdet3d', 'default_hooks': {'timer': {'type': 'IterTimerHook'}, 'logger': {'type': 'LoggerHook', 'interval': 50}, 'param_scheduler': {'type': 'ParamSchedulerHook'}, 'checkpoint': {'type': 'CheckpointHook', 'interval': -1}, 'sampler_seed': {'type': 'DistSamplerSeedHook'}, 'visualization': {'type': 'Det3DVisualizationHook'}}, 'env_cfg': {'cudnn_benchmark': False, 'mp_cfg': {'mp_start_method': 'fork', 'opencv_num_threads': 0}, 'dist_cfg': {'backend': 'nccl'}}, 'log_processor': {'type': 'LogProcessor', 'window_size': 50, 'by_epoch': True}, 'log_level': 'INFO', 'load_from': None, 'resume': False, 'max_epochs': 100, 'train_cfg': {'type': 'EpochBasedTrainLoop', 'max_epochs': 100, 'val_interval': 5}, 'val_cfg': {'type': 'ValLoop'}, 'test_cfg': {'type': 'TestLoop'}, 'param_scheduler': [{'type': 'MultiStepLR', 'begin': 0, 'end': 100, 'by_epoch': True, 'milestones': [50], 'gamma': 0.1}], 'optim_wrapper': {'type': 'OptimWrapper', 'optimizer': {'type': 'Adam', 'lr': 0.00025}, 'clip_grad': None}, 'find_unused_parameters': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = model.cfg\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the data pipeline\n",
    "test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "box_type_3d, box_mode_3d = \\\n",
    "    get_box_type(cfg.test_dataloader.dataset.box_type_3d)\n",
    "\n",
    "data_list = mmengine.load(ann_file) ##['data_list'] instead of nothing\n",
    "data_list = data_list\n",
    "assert len(imgs) == len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    LoadImageFromFileMono3D(ignore_empty=False, to_float32=False, color_type='color', imdecode_backend='cv2', backend_args=None)\n",
      "    AffineResize(img_scale=(1920, 1080), down_ratio=4) \n",
      "    Pack3DDetInputs(keys=['img'])(meta_keys=('img_path', 'ori_shape', 'img_shape', 'lidar2img', 'depth2img', 'cam2img', 'pad_shape', 'scale_factor', 'flip', 'pcd_horizontal_flip', 'pcd_vertical_flip', 'box_mode_3d', 'box_type_3d', 'img_norm_cfg', 'num_pts_feats', 'pcd_trans', 'sample_idx', 'pcd_scale_factor', 'pcd_rotation', 'pcd_rotation_angle', 'lidar_path', 'transformation_3d_flow', 'trans_mat', 'affine_aug', 'sweep_img_metas', 'ori_cam2img', 'cam2global', 'crop_offset', 'img_crop_offset', 'resize_img_shape', 'lidar2cam', 'ori_lidar2img', 'num_ref_frames', 'num_views', 'ego2global', 'axis_align_matrix'))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "img = imgs[0]\n",
    "index = 0\n",
    "# get data info containing calib\n",
    "data_info = data_list[index]\n",
    "img_path = data_info['images'][cam_type]['img_path']\n",
    "if osp.basename(img_path) != osp.basename(img):\n",
    "    raise ValueError(f'the info file of {img_path} is not provided.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002238.png\n"
     ]
    }
   ],
   "source": [
    "print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javier/sensus-loci/sensus/notebooks/002238.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# replace the img_path in data_info with img\n",
    "data_info['images'][cam_type]['img_path'] = img\n",
    "print(data_info['images'][cam_type]['img_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CAM_BACK': {'img_path': '/home/javier/sensus-loci/sensus/notebooks/002238.png', 'cam2img': [[2183.375019, 0.0, 940.590363], [0.0, 2329.297332, 567.568513], [0.0, 0.0, 1.0]]}}\n"
     ]
    }
   ],
   "source": [
    "# avoid data_info['images'] has multiple keys anout camera views.\n",
    "mono_img_info = {f'{cam_type}': data_info['images'][cam_type]}\n",
    "\n",
    "print(mono_img_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hasdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hasdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': {'CAM_BACK': {'img_path': '/home/javier/sensus-loci/sensus/notebooks/002238.png', 'cam2img': [[2183.375019, 0.0, 940.590363], [0.0, 2329.297332, 567.568513], [0.0, 0.0, 1.0]]}}, 'box_type_3d': <class 'mmdet3d.structures.bbox_3d.cam_box3d.CameraInstance3DBoxes'>, 'box_mode_3d': <Box3DMode.CAM: 1>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_ = dict(\n",
    "    images=mono_img_info,\n",
    "    box_type_3d=box_type_3d,\n",
    "    box_mode_3d=box_mode_3d)\n",
    "print(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_ = test_pipeline(data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 36,  40,  43,  ...,  66,  66,  66],\n",
      "         [ 42,  45,  48,  ...,  66,  66,  66],\n",
      "         [ 35,  42,  49,  ...,  66,  66,  66],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117, 117, 117],\n",
      "         [  7,   5,  23,  ..., 117, 117, 117],\n",
      "         [ 18,  18,  36,  ..., 117, 117, 117]],\n",
      "\n",
      "        [[ 40,  44,  47,  ...,  75,  75,  75],\n",
      "         [ 46,  49,  52,  ...,  75,  75,  75],\n",
      "         [ 39,  46,  53,  ...,  75,  75,  75],\n",
      "         ...,\n",
      "         [ 14,  10,  11,  ..., 110, 110, 110],\n",
      "         [ 17,  15,  30,  ..., 110, 110, 110],\n",
      "         [ 28,  28,  43,  ..., 110, 110, 110]],\n",
      "\n",
      "        [[ 35,  39,  42,  ...,  69,  69,  69],\n",
      "         [ 41,  44,  47,  ...,  69,  69,  69],\n",
      "         [ 34,  41,  48,  ...,  69,  69,  69],\n",
      "         ...,\n",
      "         [ 25,  21,  20,  ..., 102, 102, 102],\n",
      "         [ 29,  27,  41,  ..., 102, 102, 102],\n",
      "         [ 40,  40,  54,  ..., 102, 102, 102]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "datapip = data_['inputs']['img']\n",
    "print(datapip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 36,  40,  43,  ...,  66,  66,  66],\n",
      "         [ 42,  45,  48,  ...,  66,  66,  66],\n",
      "         [ 35,  42,  49,  ...,  66,  66,  66],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117, 117, 117],\n",
      "         [  7,   5,  23,  ..., 117, 117, 117],\n",
      "         [ 18,  18,  36,  ..., 117, 117, 117]],\n",
      "\n",
      "        [[ 40,  44,  47,  ...,  75,  75,  75],\n",
      "         [ 46,  49,  52,  ...,  75,  75,  75],\n",
      "         [ 39,  46,  53,  ...,  75,  75,  75],\n",
      "         ...,\n",
      "         [ 14,  10,  11,  ..., 110, 110, 110],\n",
      "         [ 17,  15,  30,  ..., 110, 110, 110],\n",
      "         [ 28,  28,  43,  ..., 110, 110, 110]],\n",
      "\n",
      "        [[ 35,  39,  42,  ...,  69,  69,  69],\n",
      "         [ 41,  44,  47,  ...,  69,  69,  69],\n",
      "         [ 34,  41,  48,  ...,  69,  69,  69],\n",
      "         ...,\n",
      "         [ 25,  21,  20,  ..., 102, 102, 102],\n",
      "         [ 29,  27,  41,  ..., 102, 102, 102],\n",
      "         [ 40,  40,  54,  ..., 102, 102, 102]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "video = mmcv.VideoReader('/home/javier/sensus-loci/output.mp4')\n",
    "frame = video[1050]\n",
    "# Convert numpy array to a PyTorch tensor\n",
    "torch_image = torch.tensor(frame, dtype=torch.uint8)\n",
    "torch_image = torch_image.permute(2, 0, 1)\n",
    "print(torch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if they are equal\n",
    "are_equal = torch.equal(torch_image, datapip)\n",
    "print(are_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_samples': <Det3DDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    trans_mat: array([[ 0.25, -0.  ,  0.  ],\n",
      "               [ 0.  ,  0.25,  0.  ],\n",
      "               [ 0.  ,  0.  ,  1.  ]], dtype=float32)\n",
      "    img_shape: (1080, 1920, 3)\n",
      "    ori_shape: (1080, 1920)\n",
      "    box_mode_3d: <Box3DMode.CAM: 1>\n",
      "    img_path: '/home/javier/sensus-loci/sensus/notebooks/002238.png'\n",
      "    affine_aug: False\n",
      "    pad_shape: (1080, 1920, 3)\n",
      "    box_type_3d: <class 'mmdet3d.structures.bbox_3d.cam_box3d.CameraInstance3DBoxes'>\n",
      "    cam2img: [[2183.375019, 0.0, 940.590363], [0.0, 2329.297332, 567.568513], [0.0, 0.0, 1.0]]\n",
      "\n",
      "    DATA FIELDS\n",
      "    eval_ann_info: None\n",
      "    gt_pts_seg: <PointData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7fa2f1a0ff10>\n",
      "    gt_instances_3d: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7fa34a11f220>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7fa2f1a0ffa0>\n",
      ") at 0x7fa34a11f160>, 'inputs': 'hola'}\n"
     ]
    }
   ],
   "source": [
    "data_['inputs'] = 'hola'\n",
    "print(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append(data_)\n",
    "\n",
    "collate_data = pseudo_collate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Det3DDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    cam2img: [[2183.375019, 0.0, 940.590363], [0.0, 2329.297332, 567.568513], [0.0, 0.0, 1.0]]\n",
      "    img_path: '/home/javier/sensus-loci/sensus/notebooks/002238.png'\n",
      "    pad_shape: (1088, 1920)\n",
      "    img_shape: (1080, 1920, 3)\n",
      "    trans_mat: array([[ 0.25, -0.  ,  0.  ],\n",
      "               [ 0.  ,  0.25,  0.  ],\n",
      "               [ 0.  ,  0.  ,  1.  ]], dtype=float32)\n",
      "    affine_aug: False\n",
      "    ori_shape: (1080, 1920)\n",
      "    box_type_3d: <class 'mmdet3d.structures.bbox_3d.cam_box3d.CameraInstance3DBoxes'>\n",
      "    batch_input_shape: (1088, 1920)\n",
      "    box_mode_3d: <Box3DMode.CAM: 1>\n",
      "\n",
      "    DATA FIELDS\n",
      "    eval_ann_info: None\n",
      "    gt_pts_seg: <PointData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7f6a9601d6d0>\n",
      "    pred_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7f6a945b4520>\n",
      "    pred_instances_3d: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            labels_3d: tensor([2], device='cuda:0')\n",
      "            bboxes_3d: CameraInstance3DBoxes(\n",
      "                    tensor([[10.2396, -2.6097, 45.6656,  4.3876,  1.5962,  1.8996,  2.1629]],\n",
      "                       device='cuda:0'))\n",
      "            scores_3d: tensor([0.4050], device='cuda:0')\n",
      "        ) at 0x7f6a945b40a0>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7f6a9601d8b0>\n",
      "    gt_instances_3d: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "        ) at 0x7f6a9601dcd0>\n",
      ") at 0x7f6a945b4430>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# forward the model\n",
    "with torch.no_grad():\n",
    "    results = model.test_step(collate_data)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mono_3d_detector(model: nn.Module,\n",
    "                               imgs: ImagesType,\n",
    "                               ann_file: Union[str, Sequence[str]],\n",
    "                               cam_type: str = 'CAM_FRONT'):\n",
    "    \"\"\"Inference image with the monocular 3D detector.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded detector.\n",
    "        imgs (str, Sequence[str]):\n",
    "           Either image files or loaded images.\n",
    "        ann_files (str, Sequence[str]): Annotation files.\n",
    "        cam_type (str): Image of Camera chose to infer.\n",
    "            For kitti dataset, it should be 'CAM_2',\n",
    "            and for nuscenes dataset, it should be\n",
    "            'CAM_FRONT'. Defaults to 'CAM_FRONT'.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`Det3DDataSample` or list[:obj:`Det3DDataSample`]:\n",
    "        If pcds is a list or tuple, the same length list type results\n",
    "        will be returned, otherwise return the detection results directly.\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, (list, tuple)):\n",
    "        is_batch = True\n",
    "    else:\n",
    "        imgs = [imgs]\n",
    "        is_batch = False\n",
    "\n",
    "    cfg = model.cfg\n",
    "\n",
    "    # build the data pipeline\n",
    "    test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    box_type_3d, box_mode_3d = \\\n",
    "        get_box_type(cfg.test_dataloader.dataset.box_type_3d)\n",
    "\n",
    "    data_list = mmengine.load(ann_file) ##['data_list'] instead of nothing\n",
    "    data_list = data_list\n",
    "    assert len(imgs) == len(data_list)\n",
    "\n",
    "    data = []\n",
    "    for index, img in enumerate(imgs):\n",
    "        # get data info containing calib\n",
    "        data_info = data_list[index]\n",
    "        img_path = data_info['images'][cam_type]['img_path']\n",
    "        if osp.basename(img_path) != osp.basename(img):\n",
    "            raise ValueError(f'the info file of {img_path} is not provided.')\n",
    "\n",
    "        # replace the img_path in data_info with img\n",
    "        data_info['images'][cam_type]['img_path'] = img\n",
    "        # avoid data_info['images'] has multiple keys anout camera views.\n",
    "        mono_img_info = {f'{cam_type}': data_info['images'][cam_type]}\n",
    "        data_ = dict(\n",
    "            images=mono_img_info,\n",
    "            box_type_3d=box_type_3d,\n",
    "            box_mode_3d=box_mode_3d)\n",
    "\n",
    "        data_ = test_pipeline(data_)\n",
    "        data.append(data_)\n",
    "\n",
    "    collate_data = pseudo_collate(data)\n",
    "\n",
    "    # forward the model\n",
    "    with torch.no_grad():\n",
    "        results = model.test_step(collate_data)\n",
    "\n",
    "    if not is_batch:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
