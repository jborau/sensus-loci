{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from mmdet3d.apis import inference_mono_3d_detector, init_model\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from os import path as osp\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose, pseudo_collate\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmengine.runner import load_checkpoint\n",
    "\n",
    "from mmdet3d.registry import DATASETS, MODELS\n",
    "from mmdet3d.structures import Box3DMode, Det3DDataSample, get_box_type\n",
    "from mmdet3d.structures.det3d_data_sample import SampleList\n",
    "import mmcv\n",
    "import cv2\n",
    "from sensus.tools.visualizer import ImageVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inferences(model_info, frames):\n",
    "    # model_info: dict\n",
    "    config_file = model_info['config_file']\n",
    "    checkpoint_file=model_info['checkpoint_file']\n",
    "    device=model_info['device']\n",
    "    ann_file = model_info['ann_file']\n",
    "    imgs = model_info['img_file']\n",
    "    cam_type = model_info['cam_type']\n",
    "\n",
    "    model = init_model(config_file, checkpoint_file, device=device)\n",
    "    if isinstance(imgs, (list, tuple)):\n",
    "        is_batch = True\n",
    "    else:\n",
    "        imgs = [imgs]\n",
    "        is_batch = False\n",
    "    cfg = model.cfg\n",
    "    # build the data pipeline\n",
    "    test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    box_type_3d, box_mode_3d = \\\n",
    "        get_box_type(cfg.test_dataloader.dataset.box_type_3d)\n",
    "\n",
    "    data_list = mmengine.load(ann_file)\n",
    "    assert len(imgs) == len(data_list)\n",
    "\n",
    "    data = []\n",
    "    img = imgs[0]\n",
    "    index = 0\n",
    "    # get data info containing calib\n",
    "    data_info = data_list[index]\n",
    "    img_path = data_info['images'][cam_type]['img_path']\n",
    "    if osp.basename(img_path) != osp.basename(img):\n",
    "        raise ValueError(f'the info file of {img_path} is not provided.')\n",
    "    \n",
    "    # replace the img_path in data_info with img\n",
    "    data_info['images'][cam_type]['img_path'] = img\n",
    "\n",
    "    # avoid data_info['images'] has multiple keys anout camera views.\n",
    "    mono_img_info = {f'{cam_type}': data_info['images'][cam_type]}\n",
    "\n",
    "    data_ = dict(\n",
    "        images=mono_img_info,\n",
    "        box_type_3d=box_type_3d,\n",
    "        box_mode_3d=box_mode_3d)\n",
    "    \n",
    "    data_ = test_pipeline(data_)\n",
    "    data = []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for frame in frames:\n",
    "        data_['inputs']['img'] = frame\n",
    "        data = []\n",
    "        data.append(data_)\n",
    "\n",
    "        collate_data = pseudo_collate(data)\n",
    "\n",
    "        # forward the model\n",
    "        with torch.no_grad():\n",
    "            result = model.test_step(collate_data)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    video = mmcv.VideoReader(video_path)\n",
    "    frames = []\n",
    "    frames2 = []\n",
    "    frames2.append(video[1130])\n",
    "    frames2.append(video[1250])\n",
    "    for frame in frames2:\n",
    "        # Convert numpy array to a PyTorch tensor\n",
    "        torch_image = torch.tensor(frame, dtype=torch.uint8)\n",
    "        torch_image = torch_image.permute(2, 0, 1)\n",
    "        frames.append(torch_image)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = '/home/javier/sensus-loci/sensus/notebooks/002238.pkl'\n",
    "img_path = '/home/javier/sensus-loci/sensus/notebooks/002238.png'\n",
    "cam_type = 'CAM_BACK'\n",
    "pitch = 0.2031\n",
    "\n",
    "config_file = '/home/javier/sensus-loci/sensus/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d_pretrained_kitty.py'\n",
    "checkpoint_file = '/home/javier/sensus-loci/work_dirs/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d_pretrained_kitty/epoch_100.pth'\n",
    "device = 'cuda:0'\n",
    "\n",
    "model_info = dict(\n",
    "    config_file=config_file,\n",
    "    checkpoint_file=checkpoint_file,\n",
    "    device=device,\n",
    "    ann_file=ann_file,\n",
    "    img_file=img_path,\n",
    "    cam_type=cam_type,\n",
    ")\n",
    "\n",
    "video_path = '/home/javier/sensus-loci/output.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/javier/sensus-loci/work_dirs/smoke_dla34_dlaneck_gn-all_4xb8-6x_dair-mono3d_pretrained_kitty/epoch_100.pth\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_0.projs.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_0.nodes.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.projs.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.projs.1.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.nodes.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_1.nodes.1.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.1.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.projs.2.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.1.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.dla_up.ida_2.nodes.2.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.projs.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.projs.1.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.nodes.0.conv is upgraded to version 2.\n",
      "11/03 10:50:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - ModulatedDeformConvPack neck.ida_up.nodes.1.conv is upgraded to version 2.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/javier/sensus-loci/sensus/notebooks/video.ipynb Celda 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m frames \u001b[39m=\u001b[39m process_video(video_path)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m results \u001b[39m=\u001b[39m make_inferences(model_info, frames)\n",
      "\u001b[1;32m/home/javier/sensus-loci/sensus/notebooks/video.ipynb Celda 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m imgs \u001b[39m=\u001b[39m model_info[\u001b[39m'\u001b[39m\u001b[39mimg_file\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m cam_type \u001b[39m=\u001b[39m model_info[\u001b[39m'\u001b[39m\u001b[39mcam_type\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m init_model(config_file, checkpoint_file, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(imgs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     is_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/sensus-loci/mmdetection3d/mmdet3d/apis/inference.py:107\u001b[0m, in \u001b[0;36minit_model\u001b[0;34m(config, checkpoint, device, palette, cfg_options)\u001b[0m\n\u001b[1;32m    105\u001b[0m model\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m config  \u001b[39m# save the config in the model for convenience\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mset_device(device)\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt suggest using CPU device. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    110\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mSome functions are not supported for now.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/torch/cuda/__init__.py:326\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    324\u001b[0m device \u001b[39m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 326\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_setDevice(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "frames = process_video(video_path)\n",
    "results = make_inferences(model_info, frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_results(frames, results, calib, pitch):\n",
    "    images = []\n",
    "    for i in range(len(frames)):\n",
    "        viz = ImageVisualizer()\n",
    "        img = frames[i].permute(1, 2, 0).numpy()\n",
    "        viz.image = img\n",
    "        viz.load_calib(calib)\n",
    "        result = results[i][0]\n",
    "        viz.load_results(result)\n",
    "        img = viz.draw_monodetection_results(score = 0.2, pitch = pitch, name = 'result_test_' + str(i) + '.png')\n",
    "        images.append(img)\n",
    "    print(len(images))\n",
    "    \n",
    "    # Assuming all frames have the same dimensions, get the height and width from the first frame\n",
    "    height, width, _ = images[0].shape\n",
    "    frame_size = (width, height)  # Width, Height for cv2.VideoWriter\n",
    "    frame_size = (1920, 1080)\n",
    "\n",
    "    # Define the output video path and FPS\n",
    "    output_video_path = 'output_video.mp4'\n",
    "    fps = 1  # Frames per second\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You may also use 'avc1' or 'x264' depending on your system\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for image in images:\n",
    "\n",
    "        # Display the BGR image using matplotlib (it will look incorrect in colors)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "        # Write the BGR frame to the video\n",
    "        video_writer.write(image)\n",
    "\n",
    "    # Release the VideoWriter\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/javier/sensus-loci/sensus/notebooks/video.ipynb Celda 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m calib_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/training/calib/002238.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B163.117.150.179/home/javier/sensus-loci/sensus/notebooks/video.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m draw_results(frames, results, calib_file, pitch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'draw_results' is not defined"
     ]
    }
   ],
   "source": [
    "calib_file = '/home/javier/datasets/DAIR/single-infrastructure-side-mmdet/training/calib/002238.txt'\n",
    "\n",
    "draw_results(frames, results, calib_file, pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if frames[1].is_cuda:\n",
    "        torch_image = frames[1].cpu()\n",
    "        \n",
    "# Convert it from CHW to HWC format and then to a numpy array\n",
    "numpy_image = frames[1].permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('test.png', numpy_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
