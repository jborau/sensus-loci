{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2023-04-03 23:45:54,317 - instantiator - Created a temporary directory at /tmp/tmp7756c4dn\n",
      "INFO - 2023-04-03 23:45:54,319 - instantiator - Writing /tmp/tmp7756c4dn/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Resetting default logger to print to terminal.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sensus\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "from sensus.utils.data_converter import pc2pc_object\n",
    "from open3d.web_visualizer import draw\n",
    "from mmdetection3d import data, demo, configs, checkpoints\n",
    "from sensus import configs as sensus_configs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle file lidar.pickle from data directory\n",
    "with open(os.path.join(sensus.__path__[0], 'data/lidar.pickle'), 'rb') as f:\n",
    "    lidar_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_buffer = np.frombuffer(lidar_pickle.data,\n",
    "                    dtype=[('x', np.float32), ('y', np.float32), ('z', np.float32), ('intensity', np.float32)],\n",
    "                    count=lidar_pickle.width*lidar_pickle.height, offset=0)\n",
    "pc_ros = pc_buffer.view(dtype=np.float32).reshape(pc_buffer.shape[0], -1)\n",
    "pc_ros[:, 2] = pc_ros[:, 2] + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-116.592995  , -115.0505    ,   -4.008335  ,    0.61992574],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_ros.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "std_msgs.msg.Header(stamp=builtin_interfaces.msg.Time(sec=1678708072, nanosec=670079821), frame_id='velodyne_64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_pickle.header"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.apis import inference_detector, init_model\n",
    "from mmdet3d.utils import register_all_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/messi/alvaro/sensus-loci/mmdetection3d/checkpoints/hv_second_secfpn_6x8_80e_kitti-3d-3class_20210831_022017-ae782e87.pth\n"
     ]
    }
   ],
   "source": [
    "register_all_modules()\n",
    "model_cfg = os.path.join(sensus_configs.__path__[0],\n",
    "    'second/second_hv_secfpn_8xb6-80e_kitti-3d-3class-ros.py')\n",
    "checkpoint_path = os.path.join(checkpoints.__path__[0],\n",
    "    'hv_second_secfpn_6x8_80e_kitti-3d-3class_20210831_022017-ae782e87.pth')\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = init_model(model_cfg, checkpoint_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115.05504    115.06092     -0.7852335    0.95542234]\n",
      "[-116.592995   -115.0505       -4.008335      0.61992574]\n"
     ]
    }
   ],
   "source": [
    "# Print the max and min values of each column in the point cloud\n",
    "print(np.max(pc_ros, axis=0))\n",
    "print(np.min(pc_ros, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pc_ros[:, 2], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROS\n",
    "pc_object_ros, _ = pc2pc_object(pc_ros.flatten(), model.cfg.test_pipeline)\n",
    "result_ros, _ = inference_detector(model, pc_object_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7958, 3.9464, 3.9309, 4.1214, 3.9764, 3.8614, 4.0339, 3.8712],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ros.pred_instances_3d.bboxes_3d.tensor[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z + 5\n",
    "52.1288, -30.7273,  -2.8131,   4.2216,   1.7148,   1.5176,   2.8515\n",
    "# z + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Det3DDataSample(\n",
       "\n",
       "    META INFORMATION\n",
       "    pcd_rotation: tensor([[1., 0., 0.],\n",
       "                [-0., 1., 0.],\n",
       "                [0., 0., 1.]])\n",
       "    box_type_3d: <class 'mmdet3d.structures.bbox_3d.lidar_box3d.LiDARInstance3DBoxes'>\n",
       "    pcd_rotation_angle: 0.0\n",
       "    pcd_horizontal_flip: False\n",
       "    box_mode_3d: <Box3DMode.LIDAR: 0>\n",
       "    pcd_trans: array([0., 0., 0.])\n",
       "    pcd_vertical_flip: False\n",
       "    transformation_3d_flow: ['R', 'S', 'T']\n",
       "    pcd_scale_factor: 1.0\n",
       "    flip: False\n",
       "\n",
       "    DATA FIELDS\n",
       "    pred_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "        ) at 0x7fda4b377d90>\n",
       "    gt_pts_seg: <PointData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "        ) at 0x7fda4b2c7730>\n",
       "    gt_instances_3d: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "        ) at 0x7fda4b2c73d0>\n",
       "    pred_instances_3d: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            scores_3d: tensor([0.1304, 0.1173, 0.3624, 0.3572, 0.9088, 0.8917, 0.8498, 0.7471, 0.7245,\n",
       "                        0.6414, 0.6307, 0.5603, 0.4316, 0.3937], device='cuda:0')\n",
       "            labels_3d: tensor([0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
       "            bboxes_3d: LiDARInstance3DBoxes(\n",
       "                    tensor([[ 3.0507e+01, -3.7150e+00, -1.2959e+00,  1.9228e+00,  5.8578e-01,\n",
       "                          1.7878e+00, -1.0387e-01],\n",
       "                        [ 3.3624e+01, -1.5408e+01, -1.4842e+00,  1.7356e+00,  5.3179e-01,\n",
       "                          1.7017e+00,  2.8747e+00],\n",
       "                        [ 3.0485e+01, -3.7236e+00, -1.2972e+00,  1.9133e+00,  6.0093e-01,\n",
       "                          1.7964e+00, -7.1814e-02],\n",
       "                        [ 3.3573e+01, -1.5382e+01, -1.3896e+00,  1.6332e+00,  5.3116e-01,\n",
       "                          1.6652e+00,  2.8745e+00],\n",
       "                        [ 1.4780e+01, -1.0779e+00, -1.4688e+00,  3.7479e+00,  1.5591e+00,\n",
       "                          1.4264e+00, -3.2357e-01],\n",
       "                        [ 8.1432e+00,  1.1950e+00, -1.5713e+00,  3.8181e+00,  1.5872e+00,\n",
       "                          1.5053e+00,  2.8483e+00],\n",
       "                        [ 6.5548e+00, -3.8596e+00, -1.7373e+00,  3.2281e+00,  1.4870e+00,\n",
       "                          1.4416e+00, -2.9311e-01],\n",
       "                        [ 3.3599e+01, -6.9979e+00, -1.1930e+00,  4.1792e+00,  1.6846e+00,\n",
       "                          1.5645e+00,  2.9612e+00],\n",
       "                        [ 2.0532e+01, -8.5866e+00, -1.7789e+00,  3.2964e+00,  1.5354e+00,\n",
       "                          1.5564e+00, -3.4729e-01],\n",
       "                        [ 3.6932e+00,  2.7057e+00, -1.5757e+00,  3.9363e+00,  1.6047e+00,\n",
       "                          1.4755e+00, -2.5535e-01],\n",
       "                        [ 2.8742e+01, -1.5891e+00, -9.8873e-01,  3.8667e+00,  1.5541e+00,\n",
       "                          1.4411e+00,  4.3777e+00],\n",
       "                        [ 2.4866e+01, -1.0196e+01, -1.7627e+00,  3.8253e+00,  1.5966e+00,\n",
       "                          1.5202e+00, -4.3520e-01],\n",
       "                        [ 5.5483e+01, -2.0294e+01, -1.2641e+00,  4.2528e+00,  1.6728e+00,\n",
       "                          1.5333e+00,  2.8051e+00],\n",
       "                        [ 4.0962e+01, -9.4771e+00, -1.2358e+00,  3.9789e+00,  1.6044e+00,\n",
       "                          1.5050e+00, -6.1777e-03]], device='cuda:0'))\n",
       "        ) at 0x7fda4b2c78b0>\n",
       "    eval_ann_info: None\n",
       "    gt_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "        ) at 0x7fda4b2c7dc0>\n",
       ") at 0x7fda4b377340>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.6682, -37.9861,  -2.6241,   4.3703,   1.7161,   1.5333,   3.2115],\n",
      "       device='cuda:0')\n",
      "torch.Size([18, 7])\n"
     ]
    }
   ],
   "source": [
    "# print(result_ros.pred_instances_3d.labels_3d)\n",
    "# print(result_ros.pred_instances_3d.scores_3d)\n",
    "# print(result_ros.pred_instances_3d.bboxes_3d)\n",
    "print(result_ros.pred_instances_3d.bboxes_3d.tensor[0])\n",
    "print(result_ros.pred_instances_3d.bboxes_3d.tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'rotate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Same result after processing points (maybe processing under the hood when\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# using np.array pc)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m points \u001b[39m=\u001b[39m points\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m result, data \u001b[39m=\u001b[39m inference_detector(model, points)\n",
      "File \u001b[0;32m~/alvaro/sensus-loci/mmdetection3d/mmdet3d/apis/inference.py:158\u001b[0m, in \u001b[0;36minference_detector\u001b[0;34m(model, pcds)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[39m# directly use loaded point cloud\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         data_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    152\u001b[0m             points\u001b[39m=\u001b[39mpcd,\n\u001b[1;32m    153\u001b[0m             timestamp\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m             box_type_3d\u001b[39m=\u001b[39mbox_type_3d,\n\u001b[1;32m    157\u001b[0m             box_mode_3d\u001b[39m=\u001b[39mbox_mode_3d)\n\u001b[0;32m--> 158\u001b[0m     data_ \u001b[39m=\u001b[39m test_pipeline(data_)\n\u001b[1;32m    159\u001b[0m     data\u001b[39m.\u001b[39mappend(data_)\n\u001b[1;32m    161\u001b[0m collate_data \u001b[39m=\u001b[39m pseudo_collate(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/sensus_1.1/lib/python3.8/site-packages/mmengine/dataset/base_dataset.py:58\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"Call function to apply transforms sequentially.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m   dict: Transformed data.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 58\u001b[0m     data \u001b[39m=\u001b[39m t(data)\n\u001b[1;32m     59\u001b[0m     \u001b[39m# The transform will return None when it failed to load images or\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# cannot find suitable augmentation parameters to augment the data.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# Here we simply return None if the transform returns None and the\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# dataset will handle it by randomly selecting another data sample.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sensus_1.1/lib/python3.8/site-packages/mmcv/transforms/base.py:12\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m              results: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[Dict, Tuple[List, List]]]:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(results)\n",
      "File \u001b[0;32m~/alvaro/sensus-loci/mmdetection3d/mmdet3d/datasets/transforms/test_time_aug.py:109\u001b[0m, in \u001b[0;36mMultiScaleFlipAug3D.transform\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         _results[\u001b[39m'\u001b[39m\u001b[39mpcd_horizontal_flip\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    106\u001b[0m                             pcd_horizontal_flip\n\u001b[1;32m    107\u001b[0m                         _results[\u001b[39m'\u001b[39m\u001b[39mpcd_vertical_flip\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    108\u001b[0m                             pcd_vertical_flip\n\u001b[0;32m--> 109\u001b[0m                         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(_results)\n\u001b[1;32m    110\u001b[0m                         aug_data_list\u001b[39m.\u001b[39mappend(data)\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m aug_data_list\n",
      "File \u001b[0;32m~/anaconda3/envs/sensus_1.1/lib/python3.8/site-packages/mmengine/dataset/base_dataset.py:58\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"Call function to apply transforms sequentially.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m   dict: Transformed data.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 58\u001b[0m     data \u001b[39m=\u001b[39m t(data)\n\u001b[1;32m     59\u001b[0m     \u001b[39m# The transform will return None when it failed to load images or\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# cannot find suitable augmentation parameters to augment the data.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# Here we simply return None if the transform returns None and the\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# dataset will handle it by randomly selecting another data sample.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sensus_1.1/lib/python3.8/site-packages/mmcv/transforms/base.py:12\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m              results: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[Dict, Tuple[List, List]]]:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(results)\n",
      "File \u001b[0;32m~/alvaro/sensus-loci/mmdetection3d/mmdet3d/datasets/transforms/transforms_3d.py:782\u001b[0m, in \u001b[0;36mGlobalRotScaleTrans.transform\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtransformation_3d_flow\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m input_dict:\n\u001b[1;32m    780\u001b[0m     input_dict[\u001b[39m'\u001b[39m\u001b[39mtransformation_3d_flow\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rot_bbox_points(input_dict)\n\u001b[1;32m    784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpcd_scale_factor\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m input_dict:\n\u001b[1;32m    785\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_scale(input_dict)\n",
      "File \u001b[0;32m~/alvaro/sensus-loci/mmdetection3d/mmdet3d/datasets/transforms/transforms_3d.py:725\u001b[0m, in \u001b[0;36mGlobalRotScaleTrans._rot_bbox_points\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m     input_dict[\u001b[39m'\u001b[39m\u001b[39mpoints\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m points\n\u001b[1;32m    723\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     \u001b[39m# if no bbox in input_dict, only rotate points\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m     rot_mat_T \u001b[39m=\u001b[39m input_dict[\u001b[39m'\u001b[39;49m\u001b[39mpoints\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mrotate(noise_rotation)\n\u001b[1;32m    727\u001b[0m input_dict[\u001b[39m'\u001b[39m\u001b[39mpcd_rotation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m rot_mat_T\n\u001b[1;32m    728\u001b[0m input_dict[\u001b[39m'\u001b[39m\u001b[39mpcd_rotation_angle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m noise_rotation\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'rotate'"
     ]
    }
   ],
   "source": [
    "# Same result after processing points (maybe processing under the hood when\n",
    "# using np.array pc)\n",
    "points = points.reshape(-1, 4)\n",
    "result, data = inference_detector(model, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "tensor([0.1304, 0.1173, 0.3624, 0.3572, 0.9088, 0.8917, 0.8498, 0.7471, 0.7245,\n",
      "        0.6414, 0.6307, 0.5603, 0.4316, 0.3937], device='cuda:0')\n",
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "print(result.pred_instances_3d.labels_3d)\n",
    "print(result.pred_instances_3d.scores_3d)\n",
    "print(result.pred_instances_3d.bboxes_3d.tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = os.path.join(demo.__path__[0],\n",
    "    'data/kitti/000008.bin')\n",
    "pc_path = os.path.join(\n",
    "    '/home/messi/alvaro/sensus-loci/mmdetection3d/demo/kitti_pointpillars/kitti_000008',\n",
    "    'kitti_000008_points.obj')\n",
    "bboxes_path = os.path.join(\n",
    "    '/home/messi/alvaro/sensus-loci/mmdetection3d/demo/kitti_pointpillars/kitti_000008',\n",
    "    'kitti_000008_pred.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 17238 points.\n"
     ]
    }
   ],
   "source": [
    "# Read pc from bin file\n",
    "with open(bin_path, 'rb') as f:\n",
    "    points = np.fromfile(f, dtype=np.float32, count=-1).reshape([-1, 4])\n",
    "\n",
    "pcd_bin = o3d.geometry.PointCloud()\n",
    "pcd_bin.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "print(pcd_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 16897 points.\n",
      "LineSet with 360 lines.\n"
     ]
    }
   ],
   "source": [
    "# Read pc from obj file (reading with o3d.io.read_triangle_mesh or \n",
    "# read_point_cloud does not work)\n",
    "pc = []\n",
    "with open(pc_path, 'rb') as f:\n",
    "    for each in f.readlines():\n",
    "        p1, p2, p3 = each.decode('utf-8').split(' ')[1:]\n",
    "        pc.append([float(p1), float(p2), float(p3.replace('\\n', ''))])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.array(pc))\n",
    "print(pcd)\n",
    "\n",
    "bboxes = o3d.io.read_triangle_mesh(bboxes_path)\n",
    "bboxes.compute_vertex_normals()     # For solid rendering with lighting\n",
    "bboxes_lines = o3d.geometry.LineSet().create_from_triangle_mesh(bboxes)\n",
    "bboxes_lines.paint_uniform_color([1, 0, 0])\n",
    "print(bboxes_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e26e88557d9481b915bb09e78724163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_11')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw([pcd, bboxes_lines], width=900, height=600, point_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf594293333b4974bb61a30edfe9f4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw([pcd_bin, bboxes_lines], width=900, height=600, point_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensus_1.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f415ed57821a0a57975d366e83fa539ca7287180412c6dde381c2f215d5e4022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
